<div class="cont d-flex justify-content-center align-items-center flex-column">

  <div class="custom-file">
    <input type="file" class="custom-file-input" (change)="fileChangeEvent($event)">
    <label class="custom-file-label">Choose file</label>
  </div>

  <div *ngIf="loading; else elseBlock">
    <img src="./assets/loading.gif">
  </div>

  <ng-template #elseBlock>
    <img [src]="imageSrc" #img *ngIf="imageSrc"/>
  </ng-template>

  <div class="list-group">
    <div class="list-group-item" *ngFor="let prediction of predictions">
      {{prediction.className}} - {{prediction.probability}}
    </div>
  </div>

</div>

  <script>
    // Notice there is no 'import' statement. 'tf' is available on the index-page
    // because of the script tag above.

    // custom layer
    // https://github.com/tensorflow/tfjs-examples/blob/master/custom-layer/custom_layer.js
    // https://gist.github.com/caisq/33ed021e0c7b9d0e728cb1dce399527d
    // https://github.com/tensorflow/tfjs-layers/blob/master/src/activations.ts

 


/**
 * This custom layer is similar to the 'relu' non-linear Activation `Layer`, but
 * it keeps both the negative and positive signal.  The input is centered at the
 * mean value, and then the negative activations and positive activations are
 * separated into different channels, meaning that there are twice as many
 * output channels as input channels.
 *
 * Implementing a custom `Layer` in general involves specifying a `call`
 * function, and possibly also a `computeOutputShape` and `build` function. This
 * layer does not need a custom `build` function because it does not store any
 * variables.
 *
 * Custom layers currently can not be saved / loaded.  Tracking issue at
 * https://github.com/tensorflow/tfjs/issues/254
 */
class HardSwish extends tf.layers.Layer {
  constructor() {
    super({});
    // TODO(bileschi): Can we point to documentation on masking here?
    this.supportsMasking = true;
  }

  /**
   * This layer only works on 4D Tensors [batch, height, width, channels],
   * and produces output with twice as many channels.
   *
   * layer.computeOutputShapes must be overridden in the case that the output
   * shape is not the same as the input shape.
   * @param {*} inputShapes
   */
  computeOutputShape(inputShape) {
    return [inputShape[0], inputShape[1], inputShape[2], 2 * inputShape[3]]
  }

  /**
   * Centers the input and applies the following function to every element of
   * the input.
   *
   *     x => [max(x, 0), max(-x, 0)]
   *
   * The theory being that there may be signal in the both negative and positive
   * portions of the input.  Note that this will double the number of channels.
   * @param inputs Tensor to be treated.
   * @param kwargs Only used as a pass through to call hooks.  Unused in this
   *   example code.
   */
  call(inputs, kwargs) {
    let input = inputs;
    if (Array.isArray(input)) {
      input = input[0];
    }
    this.invokeCallHook(inputs, kwargs);
    const origShape = input.shape;
    const flatShape =
        [origShape[0], origShape[1] * origShape[2] * origShape[3]];
    const flattened = input.reshape(flatShape);
    const centered = tf.sub(flattened, flattened.mean(1).expandDims(1));
    const pos = centered.relu().reshape(origShape);
    const neg = centered.neg().relu().reshape(origShape);
    return tf.concat([pos, neg], 3);
  }

  /**
   * If a custom layer class is to support serialization, it must implement
   * the `className` static getter.
   */
  static get className() {
    return 'HardSwish';
  }
}
tf.serialization.registerClass(HardSwish);  // Needed for serialization.

export function HardSwish() {
  return new HardSwish();
}




{% comment %} class HardSwish extends tf.layers.Layer {
    constructor() {
        super({});
        // TODO(bileschi): Can we point to documentation on masking here?
        this.supportsMasking = true;
    }
   
    computeOutputShape(inputShape) {
        return [inputShape[0], inputShape[1]*2 , inputShape[2] / 2];
    }
   
    call(inputs, kwargs) {
        let input = inputs;
        if (Array.isArray(input)) {
            input = input[0];
        }
        this.invokeCallHook(inputs, kwargs);
        //print('--input-shape--');
        //const input_shape = input.shape;
        //print(input_shape);
        const transpose_x = tf.transpose(input, [2,1,0]);
        //const tx_shape = transpose_x.shape;
        //print(tx_shape);
        const batchnd_x = tf.batchToSpaceND(transpose_x, [2], [[0,0]]);
        //const bx_shape = batchnd_x.shape;
        //print(bx_shape);
        const x = tf.transpose(batchnd_x , [2,1,0]);  
        return x;
    }
   
    getClassName() {
        return 'HardSwish';
    }
    
}
tf.serialization.registerClass(HardSwish); {% endcomment %}

  </script>



